Appendix
In this section, we illustrate a few examples of Docling’s output in Markdown and JSON.
Birgit Pﬁtzmann IBM Research Rueschlikon, Switzerland bpf@zurich.ibm.com
Christoph Auer IBM Research Rueschlikon, Switzerland cau@zurich.ibm.com
Michele Dolﬁ IBM Research Rueschlikon, Switzerland dol@zurich.ibm.com
Ahmed S. Nassar IBM Research Rueschlikon, Switzerland ahn@zurich.ibm.com
Peter Staar IBM Research Rueschlikon, Switzerland taa@zurich.ibm.com
Accurate document layout analysis is a key requirement for highquality PDF document conversion. With the recent availability of public, large ground-
truth datasets such as PubLayNet and DocBank, deep-learning models have proven to be very effective at layout detection and segmentation. While
these datasets are of adequate size to train such models, they severely lack in layout variability since they are sourced from scientiﬁc article
repositories such as PubMed and arXiv only. Consequently, the accuracy of the layout segmentation drops signiﬁcantly when these models are
applied on more challenging and diverse layouts. In this paper, we present DocLayNet , a new, publicly available, document-layout annotation dataset
in COCO format. It contains 80863 manually annotated pages from diverse data sources to represent a wide variability in layouts. For each PDF
page, the layout annotations provide labelled bounding-boxes with a choice of 11 distinct classes. DocLayNet also provides a subset of double- and
triple-annotated pages to determine the inter-annotator agreement. In multiple experiments, we provide baseline accuracy scores (in mAP) for a set
of popular object detection models. We also demonstrate that these models fall approximately 10% behind the inter-annotator agreement.
Furthermore, we provide evidence that DocLayNet is of sufﬁcient size. Lastly, we compare models trained on PubLayNet, DocBank and DocLayNet,
showing that layout predictions of the DocLayNettrained models are more robust and thus the preferred choice for general-purpose document-layout
analysis.
· Information systems → Document structure ; · Applied computing → Document analysis ; · Computing methodologies → Machine learning ;
Computer vision ; Object detection ;
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not
made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. Copyrights for third-party
components of this work must be honored. For all other uses, contact the owner/author(s).
KDD '22, August 14-18, 2022, Washington, DC, USA © 2022 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-9385-0/22/08.
https://doi.org/10.1145/3534678.3539043
Figure 1: Four examples of complex page layouts across different document categories
PDF document conversion, layout segmentation, object-detection, data set, Machine Learning
Birgit Pﬁtzmann, Christoph Auer, Michele Dolﬁ, Ahmed S. Nassar, and Peter Staar. 2022. DocLayNet: A Large Human-Annotated Dataset for
DocumentLayout Analysis. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '22), August 14-18,
2022, Washington, DC, USA. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/ 3534678.3539043
DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis
ABSTRACT
CCS CONCEPTS
KEYWORDS
ACM Reference Format:
DocLayNet: A Large Human-Annotated Dataset for
Document-Layout Analysis
Birgit P￿tzmann
IBM Research
Rueschlikon, Switzerland
bpf@zurich.ibm.com
Christoph Auer
IBM Research
Rueschlikon, Switzerland
cau@zurich.ibm.com
Michele Dol￿
IBM Research
Rueschlikon, Switzerland
dol@zurich.ibm.com
Ahmed S. Nassar
IBM Research
Rueschlikon, Switzerland
ahn@zurich.ibm.com
Peter Staar
IBM Research
Rueschlikon, Switzerland
taa@zurich.ibm.com
ABSTRACT
Accurate document layout analysis is a key requirement for high-
quality PDF document conversion. With the recent availability of
public, large ground-truth datasets such as PubLayNet and DocBank,
deep-learning models have proven to be very e￿ective at layout
detection and segmentation. While these datasets are of adequate
size to train such models, they severely lack in layout variability
since they are sourced from scienti￿c article repositories such as
PubMed and arXiv only. Consequently, the accuracy of the layout
segmentation drops signi￿cantly when these models are applied
on more challenging and diverse layouts. In this paper, we present
DocLayNet, a new, publicly available, document-layout annotation
dataset in COCO format. It contains 80863 manually annotated
pages from diverse data sources to represent a wide variability in
layouts. For each PDF page, the layout annotations provide labelled
bounding-boxes with a choice of 11 distinct classes. DocLayNet
also provides a subset of double- and triple-annotated pages to
determine the inter-annotator agreement. In multiple experiments,
we provide baseline accuracy scores (in mAP) for a set of popular
object detection models. We also demonstrate that these models
fall approximately 10% behind the inter-annotator agreement. Fur-
thermore, we provide evidence that DocLayNet is of su￿cient size.
Lastly, we compare models trained on PubLayNet, DocBank and
DocLayNet, showing that layout predictions of the DocLayNet-
trained models are more robust and thus the preferred choice for
general-purpose document-layout analysis.
CCS CONCEPTS
• Information systems →Document structure; • Applied com-
puting →Document analysis; • Computing methodologies
→Machine learning; Computer vision; Object detection;
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for pro￿t or commercial advantage and that copies bear this notice and the full citation
on the ￿rst page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
KDD ’22, August 14–18, 2022, Washington, DC, USA
© 2022 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-9385-0/22/08.
https://doi.org/10.1145/3534678.3539043
13
USING THE VERTICAL TUBE -
MODELS AY11230/11234
1. The vertical tube can be used for  
 
 
   instructional viewing or to photograph
 
    the image with a digital camera or a
 
 
   micro TV unit
 
2. Loosen the retention screw, then rotate 
 
    the adjustment ring to change the 
 
 
 
   length of the vertical tube.
3. Make sure that both the images in
 
OPERATION (cont.)
SELECTING OBJECTIVE 
MAGNIFICATION 
 
1. There are two objectives. The lower  
    magnification objective has a greater  
    depth of field and view.
2. In order to observe the specimen  
    easily use the lower magnification  
    objective first. Then, by rotating the  
    case, the magnification can be   
    changed.
CHANGING THE INTERPUPILLARY 
DISTANCE
1. The distance between the observer's  
    pupils is the interpupillary distance.   
2. To adjust the interpupillary distance  
    rotate the prism caps until both eyes  
    coincide with the image in the   
    eyepiece. 
 
FOCUSING
1. Remove the lens protective cover.
2. Place the specimen on the working  
    stage.
3. Focus the specimen with the left eye  
    first while turning the focus knob until  
    the image appears clear and sharp.
4. Rotate the right eyepiece ring until the  
    images in each eyepiece coincide and  
    are sharp and clear.
CHANGING THE BULB
1. Disconnect the power cord.
2. When the bulb is cool, remove the  
    oblique illuminator cap and remove  
    the halogen bulb with cap.
3. Replace with a new halogen bulb.
4. Open the window in the base plate and  
    replace the halogen lamp or  
 
    fluorescent lamp of transmitted   
    illuminator.
FOCUSING
1. Turn the focusing knob away or toward  
    you until a clear image is viewed.
2. If the image is unclear, adjust the  
    height of the elevator up or down,  
    then turn the focusing knob again.
ZOOM MAGNIFICATION
1. Turn the zoom magnification knob to  
    the desired magnification and field of  
    view.
2. In most situations, it is recommended  
    that you focus at the lowest  
 
    magnification, then move to a higher  
    magnification and re-focus as  
 
    necessary.
3. If the image is not clear to both eyes  
    at the same time, the diopter ring may  
    need adjustment.
DIOPTER RING ADJUSTMENT
1. To adjust the eyepiece for viewing with  
    or without eyeglasses and for  
 
    differences in acuity between the right  
    and left eyes, follow the following  
    steps:
    a. Observe an image through the left  
        eyepiece and bring a specific point  
        into focus using the focus knob.
    b. By turning the diopter ring  
 
        adjustment for the left eyepiece,  
        bring the same point into sharp  
        focus.
     c.Then bring the same point into  
        focus through the right eyepiece   
        by turning the right diopter ring.
     d.With more than one viewer, each  
        viewer should note their own   
        diopter ring position for the left   
        and right eyepieces, then before  
        viewing set the diopter ring   
        adjustments to that setting.
CHANGING THE BULB
1. Disconnect the power cord from the  
    electrical outlet.
2. When the bulb is cool, remove the  
    oblique illuminator cap and remove  
    the halogen bulb with cap.
3. Replace with a new halogen bulb.
4. Open the window in the base plate   
    and replace the halogen lamp or  
    fluorescent lamp of transmitted   
    illuminator.
 
 
 
 
 
Model AY11230
Model AY11234
14
Objectives
Revolving Turret
Coarse 
Adjustment
Knob
MODEL AY11236
MICROSCOPE USAGE
BARSKA Model AY11236 is a powerful fixed power compound 
microscope designed for biological studies such as specimen 
examination. It can also be used for examining bacteria and          
for general clinical and medical studies and other scientific uses. 
CONSTRUCTION
BARSKA Model AY11236 is a fixed power compound microscope.   
It is constructed with two optical paths at the same angle. It is 
equipped with transmitted illumination. By using this instrument, 
the user can observe specimens at magnification from 40x to 
1000x by selecting the desired objective lens. Coarse and fine 
focus adjustments provide accuracy and image detail. The rotating 
head allows the user to position the eyepieces for maximum 
viewing comfort and easy access to all adjustment knobs.  
Model AY11236
Fine 
Adjustment
Knob
Stage
Condenser 
Focusing
Knob
Eyepiece
Stand
Lamp 
On/Off
Switch
Lamp 
Power
Cord
Rotating Head
Stage Clip
Adjustment
Interpupillary Slide Adjustment
Circling Minimums
7KHUHZDVDFKDQJHWRWKH7(536FULWHULDLQWKDWDႇHFWVFLUFOLQJDUHDGLPHQVLRQE\H[SDQGLQJWKHDUHDVWRSURYLGH
improved obstacle protection. To indicate that the new criteria had been applied to a given procedure, a 
 is placed on 
the circling line of minimums. The new circling tables and explanatory information is located in the Legend of the TPP.
7KHDSSURDFKHVXVLQJVWDQGDUGFLUFOLQJDSSURDFKDUHDVFDQEHLGHQWL¿HGE\WKHDEVHQFHRIWKH
 on the circling line of 
minima.
$SSO\6WDQGDUG&LUFOLQJ$SSURDFK0DQHXYHULQJ5DGLXV7DEOH
$SSO\([SDQGHG&LUFOLQJ$SSURDFK0DQHXYHULQJ$LUVSDFH5DGLXV
Table
AIRPORT SKETCH
 
 
                                                                                                                            
The airport sketch is a depiction of the airport with emphasis on runway pattern and related 
information, positioned in either the lower left or lower right corner of the chart to aid pi-
lot recognition of the airport from the air and to provide some information to aid on ground 
navigation of the airport. The runways are drawn to scale and oriented to true north. Runway 
dimensions (length and width) are shown for all active runways.
Runway(s) are depicted based on what type and construction of the runway.
Hard Surface
Other Than 
Hard Surface
Metal Surface
Closed Runway
Under Construction
Stopways, 
Taxiways, Park-
ing Areas
Displaced 
Threshold
Closed  
Pavement
Water Runway
Taxiways and aprons are shaded grey. Other runway features that may be shown are runway numbers, runway dimen-
sions, runway slope, arresting gear, and displaced threshold.
2WKHULQIRUPDWLRQFRQFHUQLQJOLJKWLQJ¿QDODSSURDFKEHDULQJVDLUSRUWEHDFRQREVWDFOHVFRQWUROWRZHU1$9$,'VKHOL-
pads may also be shown.
$LUSRUW(OHYDWLRQDQG7RXFKGRZQ=RQH(OHYDWLRQ
The airport elevation is shown enclosed within a box in the upper left corner of the sketch box and the touchdown zone 
elevation (TDZE) is shown in the upper right corner of the sketch box. The airport elevation is the highest point of an 
DLUSRUW¶VXVDEOHUXQZD\VPHDVXUHGLQIHHWIURPPHDQVHDOHYHO7KH7'=(LVWKHKLJKHVWHOHYDWLRQLQWKH¿UVWIHHWRI
the landing surface. Circling only approaches will not show a TDZE.
114
FAA Chart Users’ Guide - Terminal Procedures Publication (TPP) - Terms
AGL 2013 Financial Calendar
22 August 2012 
2012 full year result and ﬁnal dividend announced
30 August 2012 
Ex-dividend trading commences
5 September 2012 
Record date for 2012 ﬁnal dividend
27 September 2012 
Final dividend payable
23 October 2012 
Annual General Meeting
27 February 20131 
2013 interim result and interim dividend announced
28 August 20131 
2013 full year results and ﬁnal dividend announced 
1 Indicative dates only, subject to change/Board conﬁrmation
AGL’s Annual General Meeting will be held at the City Recital Hall, Angel Place, Sydney 
commencing at 10.30am on Tuesday 23 October 2012.
Yesterday
Established in Sydney in 1837, and then 
known as The Australian Gas Light Company, 
the AGL business has an established history 
and reputation for serving the gas and 
electricity needs of Australian households. 
In 1841, when AGL supplied the gas to light 
the ﬁrst public street lamp, it was reported 
in the Sydney Gazette as a “wonderful 
achievement of scientiﬁc knowledge, assisted 
by mechanical ingenuity.” Within two years, 
165 gas lamps were lighting the City of Sydney.
Looking back on 
175 years of 
looking forward.
AGL Energy Limited ABN 74 115 061 375
29
signs, signals and road markings
3
In chapter 2, you and your vehicle, you learned about 
some of the controls in your vehicle. This chapter is a handy 
reference section that gives examples of the most common 
signs, signals and road markings that keep trafﬁc organized 
and ﬂowing smoothly. 
Signs
There are three ways to read signs: by their shape, colour and 
the messages printed on them. Understanding these three ways 
of classifying signs will help you ﬁgure out the meaning of signs 
that are new to you. 
Stop
Yield the right-of-way
Shows driving  
regulations
Explains lane use
School zone signs 
are ﬂuorescent 
yellow-green
Tells about motorist 
services
Shows a permitted 
action
Shows an action that 
is not permitted
Warns of hazards 
ahead
Warns of  
construction zones
Railway crossing
Shows distance and 
direction
• Signs
 – regulatory signs
 – school, 
playground and 
crosswalk signs
 – lane use signs
 –  turn control signs
 –  parking signs
 –  reserved lane 
signs
 –  warning signs
 –  object markers
 –  construction 
signs
 – information and 
destination signs
 –  railway signs
• Signals
 – lane control 
signals
 – trafﬁc lights
• Road markings
 – yellow lines
 – white lines
 – reserved lane 
markings
 – other markings
in this chapter
Figure 1: Four examples of complex page layouts across dif-
ferent document categories
KEYWORDS
PDF document conversion, layout segmentation, object-detection,
data set, Machine Learning
ACM Reference Format:
Birgit P￿tzmann, Christoph Auer, Michele Dol￿, Ahmed S. Nassar, and Peter
Staar. 2022. DocLayNet: A Large Human-Annotated Dataset for Document-
Layout Analysis. In Proceedings of the 28th ACM SIGKDD Conference on
Knowledge Discovery and Data Mining (KDD ’22), August 14–18, 2022, Wash-
ington, DC, USA. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/
3534678.3539043
arXiv:2206.01062v1  [cs.CV]  2 Jun 2022
Figure 2: Title page of the DocLayNet paper (arxiv.org/pdf/2206.01062) - left PDF, right rendered
Markdown. If recognized, metadata such as authors are appearing first under the title. Text content
inside figures is currently dropped, the caption is retained and linked to the figure in the JSON
representation (not shown).
7
